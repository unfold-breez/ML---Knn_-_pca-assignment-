{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{ “nbformat”: 4, “nbformat_minor”: 0, “metadata”: { “colab”: {\n",
    "“provenance”: \\[\\] }, “kernelspec”: { “name”: “python3”, “display_name”:\n",
    "“Python 3” }, “language_info”: { “name”: “python” } }, “cells”: \\[ {\n",
    "“cell_type”: “markdown”, “source”: \\[ “Assignment Code: DA-AG-016”, “”,\n",
    "“#KNN & PCA \\| Assignment” \\], “metadata”: { “id”: “9yy7jvmUOSz-” } }, {\n",
    "“cell_type”: “markdown”, “source”: \\[ “#Question 1: What is K-Nearest\n",
    "Neighbors (KNN) and how does it work in both classification and\n",
    "regression problems?”, “”, “**K-Nearest Neighbors (KNN)** is a\n",
    "**non-parametric, instance-based machine learning algorithm** that can\n",
    "be used for both **classification** and **regression** tasks. It makes\n",
    "predictions based on the similarity between a new data point and the\n",
    "points in the training dataset.”, “”, “\\## **How KNN Works**”, “”, “1.\n",
    "**Store the training data**”, ” KNN is **lazy learning**, meaning it\n",
    "doesn’t build an explicit model. It just stores the training\n",
    "data.“,”“,”2. **Compute distance to neighbors**“,” When making a\n",
    "prediction for a new input, KNN calculates the distance between the new\n",
    "point and all training points.“,”“,” \\* Common distance metrics:\n",
    "**Euclidean**, **Manhattan**, or **Minkowski distance**.“,”“,”3.\n",
    "**Select K nearest neighbors**“,”“,” \\* Choose the **K closest points**\n",
    "from the training data based on the distance metric.“,”“,”4. **Predict\n",
    "output**“,”“,” \\* **Classification:**“,”“,” \\* Count the classes of the\n",
    "K neighbors.“,” \\* Assign the class that occurs most frequently\n",
    "(majority voting).“,” \\* Example: If 3 neighbors are \"A\" and 2 are \"B\",\n",
    "predict \"A\".“,” \\* **Regression:**“,”“,” \\* Take the **average (or\n",
    "weighted average)** of the K neighbors’ target values.“,” \\* Example: If\n",
    "neighbors’ values are 10, 12, 15 → predict\n",
    "`(10+12+15)/3 = 12.33`.“,”“,”\\## **Key Points**“,”“,”\\* **Choice of\n",
    "K:**“,”“,” \\* Small K → sensitive to noise (high variance).“,” \\* Large\n",
    "K → smoother predictions but may miss local patterns (high bias).“,”\\*\n",
    "**Feature scaling matters:**“,”“,” \\* KNN is distance-based, so features\n",
    "should often be normalized or standardized.“,”\\*\n",
    "**Non-parametric:**“,”“,” \\* KNN does not assume a particular\n",
    "distribution for the data.“,”“,”**In short:**“,”“,”\\* KNN **classifies**\n",
    "a point based on the majority class of its nearest neighbors.“,”\\* KNN\n",
    "**regresses** a point based on the average value of its nearest\n",
    "neighbors.“,”“,”—“,”“,”#Question 2: What is the Curse of Dimensionality\n",
    "and how does it affect KNN performance?“,”“,”The **Curse of\n",
    "Dimensionality** refers to the problems that arise when working with\n",
    "**high-dimensional data**—i.e., datasets with a very large number of\n",
    "features. It particularly affects algorithms like **K-Nearest Neighbors\n",
    "(KNN)** that rely on distance metrics.“,”“,”\\## **Why it\n",
    "Happens**“,”“,”1. In high-dimensional space, the volume of the feature\n",
    "space grows exponentially.“,”2. Data points become **sparse**, and the\n",
    "concept of “nearest neighbors” becomes less meaningful.“,”3. The\n",
    "difference in distance between the nearest and farthest points shrinks,\n",
    "making it hard for KNN to distinguish close points from distant\n",
    "ones.“,”“,”\\## **Effects on KNN Performance**“,”“,”1. **Distance metrics\n",
    "lose significance**“,”“,” \\* KNN relies on distances (e.g., Euclidean)\n",
    "to find neighbors.“,” \\* In high dimensions, all points tend to have\n",
    "similar distances from each other, so KNN struggles to identify truly\n",
    "“nearest” neighbors.“,”“,”2. **Increased overfitting**“,”“,” \\* With\n",
    "many dimensions, KNN may pick neighbors that are not truly similar in a\n",
    "meaningful way, leading to noisy predictions.“,”“,”3. **Higher\n",
    "computational cost**“,”“,” \\* Distance calculations increase linearly\n",
    "with the number of features, slowing down predictions.“,”“,”\\## **Ways\n",
    "to Mitigate**“,”“,”\\* **Dimensionality reduction**:“,”“,” \\* Use\n",
    "techniques like **PCA**, **t-SNE**, or **feature selection** to reduce\n",
    "the number of features.“,”\\* **Distance weighting**:“,”“,” \\* Give\n",
    "closer neighbors more weight to reduce the impact of distant irrelevant\n",
    "points.“,”\\* **Use tree-based or other algorithms**:“,”“,” \\* In very\n",
    "high-dimensional data, algorithms less sensitive to distances (like\n",
    "Random Forests or Gradient Boosting) may perform\n",
    "better.“,”“,”—“,”“,”#Question 3: What is Principal Component Analysis\n",
    "(PCA)? How is it different from feature selection?“,”“,”**Principal\n",
    "Component Analysis (PCA)** is a **dimensionality reduction technique**\n",
    "used in machine learning and statistics. Its goal is to reduce the\n",
    "number of features in a dataset while retaining as much variance\n",
    "(information) as possible.“,”“,”\\## **How PCA Works**“,”“,”1.\n",
    "**Standardize the data**“,”“,” \\* Scale features so that each has mean 0\n",
    "and standard deviation 1 (important if features have different\n",
    "scales).“,”“,”2. **Compute covariance matrix**“,”“,” \\* Measures how\n",
    "features vary together.“,”“,”3. **Calculate eigenvectors and\n",
    "eigenvalues**“,”“,” \\* Eigenvectors (principal components) define new\n",
    "axes in the feature space.“,” \\* Eigenvalues indicate how much variance\n",
    "is captured by each component.“,”“,”4. **Select top principal\n",
    "components**“,”“,” \\* Keep the first **k components** that capture most\n",
    "of the variance.“,”“,”5. **Transform the original data**“,”“,” \\*\n",
    "Project data onto these components, reducing dimensions while retaining\n",
    "essential patterns.“,”“,”\\## **PCA vs Feature Selection**“,”“,”\\| Aspect\n",
    "\\| PCA \\| Feature Selection \\|“,”\\| —————————- \\|\n",
    "—————————————————————————————– \\| —————————————————————————— \\|“,”\\|\n",
    "**Goal** \\| Reduce dimensions by creating **new features** (linear\n",
    "combinations of original features) \\| Reduce dimensions by **selecting a\n",
    "subset of existing features** \\|“,”\\| **Method** \\| Transformative /\n",
    "unsupervised \\| Filtering, wrapper, or embedded methods (can be\n",
    "supervised) \\|“,”\\| **Feature interpretability** \\| Principal components\n",
    "are combinations, harder to interpret \\| Selected features are original,\n",
    "easy to interpret \\|“,”\\| **Variance retention** \\| Retains maximum\n",
    "variance \\| Retains most predictive power depending on method \\|“,”\\|\n",
    "**Example** \\| PCA reduces 100 features to 10 components \\| Select top\n",
    "10 most important features based on correlation or model importance\n",
    "\\|“,”“,”—“,”“,”#Question 4: What are eigenvalues and eigenvectors in\n",
    "PCA, and why are they important?“,”“,”In **Principal Component Analysis\n",
    "(PCA)**, **eigenvalues** and **eigenvectors** are fundamental because\n",
    "they define the **principal components**, which determine how the data\n",
    "is transformed and reduced in dimensions.“,”“,”\\## **1.\n",
    "Eigenvectors**“,”“,”\\* An **eigenvector** is a direction in the feature\n",
    "space along which the data varies the most.“,”\\* In PCA, eigenvectors of\n",
    "the covariance matrix represent the **axes of the new feature space**\n",
    "(principal components).“,”\\* They are **unit vectors** that point in the\n",
    "directions where the data has maximum\n",
    "variance.“,”“,”**Intuition:**“,”“,”\\* If your dataset is like a cloud of\n",
    "points, eigenvectors are the directions along which the cloud stretches\n",
    "the most.“,”“,”\\## **2. Eigenvalues**“,”“,”\\* An **eigenvalue**\n",
    "corresponds to an eigenvector and indicates **how much variance is\n",
    "captured along that direction**.“,”\\* Larger eigenvalues → more variance\n",
    "captured → more “information” along that component.“,”\\* PCA ranks\n",
    "principal components by eigenvalues to decide which components to\n",
    "keep.“,”“,”**Intuition:**“,”“,”\\* Think of eigenvalue as the\n",
    "“importance” of the corresponding eigenvector: higher means that\n",
    "direction explains more of the data’s spread.“,”“,”\\## **Why They Are\n",
    "Important in PCA**“,”“,”1. **Determine principal components**“,”“,” \\*\n",
    "Eigenvectors define the directions of the new axes (components).“,” \\*\n",
    "Eigenvalues rank these axes by importance.“,”“,”2. **Dimensionality\n",
    "reduction**“,”“,” \\* By keeping the eigenvectors with the largest\n",
    "eigenvalues, you retain most of the variance while reducing the number\n",
    "of features.“,”“,”3. **Data transformation**“,”“,” \\* Projecting data\n",
    "onto eigenvectors (principal components) creates uncorrelated features\n",
    "that summarize the original data efficiently.“,”“,”—“,”“,”#Question 5:\n",
    "How do KNN and PCA complement each other when applied in a single\n",
    "pipeline?“,”“,”Here’s how **KNN and PCA complement each other** with the\n",
    "**Wine dataset** and a Python example showing a full pipeline:“,”“,”\\##\n",
    "**Complementarity**“,”“,”1. **PCA reduces dimensionality**“,”“,” \\* The\n",
    "Wine dataset has 13 numeric features.“,” \\* PCA projects them onto fewer\n",
    "components that retain most of the variance, removing redundant or noisy\n",
    "information.“,”“,”2. **KNN is distance-based**“,”“,” \\* KNN relies on\n",
    "distances between points.“,” \\* Fewer, meaningful dimensions from PCA\n",
    "make these distances more reliable, avoiding the curse of\n",
    "dimensionality.“,”“,”3. **Efficiency and accuracy**“,”“,” \\* Reduced\n",
    "dimensions → faster KNN computations.“,” \\* Focused features → better\n",
    "neighbor selection → improved classification accuracy.“,”“,”\\## **Python\n",
    "Example: KNN + PCA on Wine\n",
    "Dataset**“,”“,”`python\\n\",         \"# Import libraries\\n\",         \"from sklearn.datasets import load_wine\\n\",         \"from sklearn.model_selection import train_test_split\\n\",         \"from sklearn.preprocessing import StandardScaler\\n\",         \"from sklearn.decomposition import PCA\\n\",         \"from sklearn.neighbors import KNeighborsClassifier\\n\",         \"from sklearn.pipeline import Pipeline\\n\",         \"from sklearn.metrics import accuracy_score\\n\",         \"\\n\",         \"# Load dataset\\n\",         \"data = load_wine()\\n\",         \"X = data.data\\n\",         \"y = data.target\\n\",         \"\\n\",         \"# Split dataset\\n\",         \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\",         \"\\n\",         \"# Create a pipeline: Standardization -> PCA -> KNN\\n\",         \"pipeline = Pipeline([\\n\",         \"    ('scaler', StandardScaler()),\\n\",         \"    ('pca', PCA(n_components=5)),  # Reduce to 5 principal components\\n\",         \"    ('knn', KNeighborsClassifier(n_neighbors=5))\\n\",         \"])\\n\",         \"\\n\",         \"# Train the pipeline\\n\",         \"pipeline.fit(X_train, y_train)\\n\",         \"\\n\",         \"# Predict\\n\",         \"y_pred = pipeline.predict(X_test)\\n\",         \"\\n\",         \"# Evaluate accuracy\\n\",         \"accuracy = accuracy_score(y_test, y_pred)\\n\",         \"print(f\\\"KNN with PCA Accuracy: {accuracy:.4f}\\\")\\n\",         \"`“,”“,”**Explanation:**“,”“,”\\*\n",
    "**StandardScaler**: Standardizes features before PCA.“,”\\*\n",
    "\\*\\*PCA(n\\\\\\_components=5)**: Keeps the 5 components that capture most\n",
    "variance.“,”\\* **KNeighborsClassifier**: Runs KNN in the reduced PCA\n",
    "space.“,”\\* **Pipeline\\*\\*: Ensures all steps are applied consistently\n",
    "to train and test data.” \\], “metadata”: { “id”: “3eE0UERJOV9G” } }, {\n",
    "“cell_type”: “markdown”, “source”: \\[ “#Question 6: Train a KNN\n",
    "Classifier on the Wine dataset with and without feature scaling. Compare\n",
    "model accuracy in both cases.”, “” \\], “metadata”: { “id”:\n",
    "“c7RRZMIyRhsj” } }, { “cell_type”: “code”, “source”: \\[ “\\# Import\n",
    "libraries”, “from sklearn.datasets import load_wine”, “from\n",
    "sklearn.model_selection import train_test_split”, “from\n",
    "sklearn.preprocessing import StandardScaler”, “from sklearn.neighbors\n",
    "import KNeighborsClassifier”, “from sklearn.metrics import\n",
    "accuracy_score”, “”, “\\# Load dataset”, “data = load_wine()”, “X =\n",
    "data.data”, “y = data.target”, “”, “\\# Split dataset”, “X_train, X_test,\n",
    "y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)”, “”, “\\# ——— KNN without scaling ———”, “knn_no_scaling\n",
    "= KNeighborsClassifier(n_neighbors=5)”, “knn_no_scaling.fit(X_train,\n",
    "y_train)”, “y_pred_no_scaling = knn_no_scaling.predict(X_test)”,\n",
    "“accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)”,\n",
    "“print(f\"KNN Accuracy without scaling: {accuracy_no_scaling:.4f}\")”, “”,\n",
    "“\\# ——— KNN with feature scaling ———”, “scaler = StandardScaler()”,\n",
    "“X_train_scaled = scaler.fit_transform(X_train)”, “X_test_scaled =\n",
    "scaler.transform(X_test)”, “”, “knn_scaled =\n",
    "KNeighborsClassifier(n_neighbors=5)”, “knn_scaled.fit(X_train_scaled,\n",
    "y_train)”, “y_pred_scaled = knn_scaled.predict(X_test_scaled)”,\n",
    "“accuracy_scaled = accuracy_score(y_test, y_pred_scaled)”, “print(f\"KNN\n",
    "Accuracy with scaling: {accuracy_scaled:.4f}\")” \\], “metadata”: {\n",
    "“colab”: { “base_uri”: “https://localhost:8080/” }, “id”:\n",
    "“M1NAnVjVRyqb”, “outputId”: “d7523a7f-e6b1-4825-fdbb-c5c43c6d523a” },\n",
    "“execution_count”: null, “outputs”: \\[ { “output_type”: “stream”,\n",
    "“name”: “stdout”, “text”: \\[ “KNN Accuracy without scaling: 0.7222”,\n",
    "“KNN Accuracy with scaling: 0.9444” \\] } \\] }, { “cell_type”:\n",
    "“markdown”, “source”: \\[ “#Question 7: Train a PCA model on the Wine\n",
    "dataset and print the explained variance ratio of each principal\n",
    "component.” \\], “metadata”: { “id”: “xDj00X-4R3Eb” } }, { “cell_type”:\n",
    "“code”, “source”: \\[ “\\# Import libraries”, “from sklearn.datasets\n",
    "import load_wine”, “from sklearn.preprocessing import StandardScaler”,\n",
    "“from sklearn.decomposition import PCA”, “”, “\\# Load dataset”, “data =\n",
    "load_wine()”, “X = data.data”, “”, “\\# Standardize features”, “scaler =\n",
    "StandardScaler()”, “X_scaled = scaler.fit_transform(X)”, “”, “\\# Train\n",
    "PCA model”, “pca = PCA()”, “X_pca = pca.fit_transform(X_scaled)”, “”,\n",
    "“\\# Print explained variance ratio”, “explained_variance =\n",
    "pca.explained_variance_ratio\\_”, “for i, ratio in\n",
    "enumerate(explained_variance, start=1):”, ” print(f\"Principal Component\n",
    "{i}: {ratio:.4f}\")“,”“,”\\# Optionally, print cumulative\n",
    "variance“,”cumulative_variance = explained_variance.cumsum()“,”for i,\n",
    "cum_var in enumerate(cumulative_variance, start=1):“,”\n",
    "print(f\"Cumulative variance after PC{i}: {cum_var:.4f}\")” \\],\n",
    "“metadata”: { “colab”: { “base_uri”: “https://localhost:8080/” }, “id”:\n",
    "“jrafNQsiRzlT”, “outputId”: “66de174a-739e-48a1-807c-f05b4a3df3a7” },\n",
    "“execution_count”: null, “outputs”: \\[ { “output_type”: “stream”,\n",
    "“name”: “stdout”, “text”: \\[ “Principal Component 1: 0.3620”, “Principal\n",
    "Component 2: 0.1921”, “Principal Component 3: 0.1112”, “Principal\n",
    "Component 4: 0.0707”, “Principal Component 5: 0.0656”, “Principal\n",
    "Component 6: 0.0494”, “Principal Component 7: 0.0424”, “Principal\n",
    "Component 8: 0.0268”, “Principal Component 9: 0.0222”, “Principal\n",
    "Component 10: 0.0193”, “Principal Component 11: 0.0174”, “Principal\n",
    "Component 12: 0.0130”, “Principal Component 13: 0.0080”, “Cumulative\n",
    "variance after PC1: 0.3620”, “Cumulative variance after PC2: 0.5541”,\n",
    "“Cumulative variance after PC3: 0.6653”, “Cumulative variance after PC4:\n",
    "0.7360”, “Cumulative variance after PC5: 0.8016”, “Cumulative variance\n",
    "after PC6: 0.8510”, “Cumulative variance after PC7: 0.8934”, “Cumulative\n",
    "variance after PC8: 0.9202”, “Cumulative variance after PC9: 0.9424”,\n",
    "“Cumulative variance after PC10: 0.9617”, “Cumulative variance after\n",
    "PC11: 0.9791”, “Cumulative variance after PC12: 0.9920”, “Cumulative\n",
    "variance after PC13: 1.0000” \\] } \\] }, { “cell_type”: “markdown”,\n",
    "“source”: \\[ “#Question 8: Train a KNN Classifier on the PCA-transformed\n",
    "dataset (retain top 2 components). Compare the accuracy with the\n",
    "original dataset.” \\], “metadata”: { “id”: “rPYhVpbuSFb\\_” } }, {\n",
    "“cell_type”: “code”, “source”: \\[ “\\# Import libraries”, “from\n",
    "sklearn.datasets import load_wine”, “from sklearn.model_selection import\n",
    "train_test_split”, “from sklearn.preprocessing import StandardScaler”,\n",
    "“from sklearn.decomposition import PCA”, “from sklearn.neighbors import\n",
    "KNeighborsClassifier”, “from sklearn.metrics import accuracy_score”, “”,\n",
    "“\\# Load dataset”, “data = load_wine()”, “X = data.data”, “y =\n",
    "data.target”, “”, “\\# Split dataset”, “X_train, X_test, y_train, y_test\n",
    "= train_test_split(X, y, test_size=0.2, random_state=42)”, “”, “\\#\n",
    "——————– KNN on Original Data ——————–”, “scaler = StandardScaler()”,\n",
    "“X_train_scaled = scaler.fit_transform(X_train)”, “X_test_scaled =\n",
    "scaler.transform(X_test)”, “”, “knn_original =\n",
    "KNeighborsClassifier(n_neighbors=5)”, “knn_original.fit(X_train_scaled,\n",
    "y_train)”, “y_pred_original = knn_original.predict(X_test_scaled)”,\n",
    "“accuracy_original = accuracy_score(y_test, y_pred_original)”,\n",
    "“print(f\"KNN Accuracy on Original Data: {accuracy_original:.4f}\")”, “”,\n",
    "“\\# ——————– KNN on PCA-transformed Data ——————–”, “\\# Apply PCA to\n",
    "reduce to 2 components”, “pca = PCA(n_components=2)”, “X_train_pca =\n",
    "pca.fit_transform(X_train_scaled)”, “X_test_pca =\n",
    "pca.transform(X_test_scaled)”, “”, “knn_pca =\n",
    "KNeighborsClassifier(n_neighbors=5)”, “knn_pca.fit(X_train_pca,\n",
    "y_train)”, “y_pred_pca = knn_pca.predict(X_test_pca)”, “accuracy_pca =\n",
    "accuracy_score(y_test, y_pred_pca)”, “print(f\"KNN Accuracy on\n",
    "PCA-transformed Data (2 components): {accuracy_pca:.4f}\")” \\],\n",
    "“metadata”: { “colab”: { “base_uri”: “https://localhost:8080/” }, “id”:\n",
    "“0U5WMFO-SDaE”, “outputId”: “d25d6bc8-dceb-444c-cda8-d707bfa575eb” },\n",
    "“execution_count”: null, “outputs”: \\[ { “output_type”: “stream”,\n",
    "“name”: “stdout”, “text”: \\[ “KNN Accuracy on Original Data: 0.9444”,\n",
    "“KNN Accuracy on PCA-transformed Data (2 components): 1.0000” \\] } \\] },\n",
    "{ “cell_type”: “markdown”, “source”: \\[ “#Question 9: Train a KNN\n",
    "Classifier with different distance metrics (`euclidean`, `manhattan`) on\n",
    "the scaled Wine dataset and compare the results.” \\], “metadata”: {\n",
    "“id”: “k77WSXg0SjDY” } }, { “cell_type”: “code”, “source”: \\[ “\\# Import\n",
    "libraries”, “from sklearn.datasets import load_wine”, “from\n",
    "sklearn.model_selection import train_test_split”, “from\n",
    "sklearn.preprocessing import StandardScaler”, “from sklearn.neighbors\n",
    "import KNeighborsClassifier”, “from sklearn.metrics import\n",
    "accuracy_score”, “”, “\\# Load dataset”, “data = load_wine()”, “X =\n",
    "data.data”, “y = data.target”, “”, “\\# Split dataset”, “X_train, X_test,\n",
    "y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)”, “”, “\\# Scale features”, “scaler = StandardScaler()”,\n",
    "“X_train_scaled = scaler.fit_transform(X_train)”, “X_test_scaled =\n",
    "scaler.transform(X_test)”, “”, “\\# Distance metrics to test”, “metrics =\n",
    "\\[‘euclidean’, ‘manhattan’\\]”, “”, “for metric in metrics:”, ” \\#\n",
    "Initialize KNN with the distance metric“,” knn =\n",
    "KNeighborsClassifier(n_neighbors=5, metric=metric)“,” \\# Train the\n",
    "model“,” knn.fit(X_train_scaled, y_train)“,” \\# Predict on test set“,”\n",
    "y_pred = knn.predict(X_test_scaled)“,” \\# Calculate accuracy“,” accuracy\n",
    "= accuracy_score(y_test, y_pred)“,” print(f\"KNN Accuracy with {metric}\n",
    "distance: {accuracy:.4f}\")” \\], “metadata”: { “colab”: { “base_uri”:\n",
    "“https://localhost:8080/” }, “id”: “57O4t2ecSh0R”, “outputId”:\n",
    "“b4227f96-1bcf-43f1-ba90-6a6138f30295” }, “execution_count”: null,\n",
    "“outputs”: \\[ { “output_type”: “stream”, “name”: “stdout”, “text”: \\[\n",
    "“KNN Accuracy with euclidean distance: 0.9444”, “KNN Accuracy with\n",
    "manhattan distance: 0.9444” \\] } \\] }, { “cell_type”: “markdown”,\n",
    "“source”: \\[ “#Question 10: You are working with a high-dimensional gene\n",
    "expression dataset to classify patients with different types of\n",
    "cancer.”, “”, “Due to the large number of features and a small number of\n",
    "samples, traditional models overfit.”, “”, “Explain how you would:”, “”,\n",
    "“● Use PCA to reduce dimensionality”, “”, “● Decide how many components\n",
    "to keep”, “”, “● Use KNN for classification post-dimensionality\n",
    "reduction”, “● Evaluate the model”, “”, “● Justify this pipeline to your\n",
    "stakeholders as a robust solution for real-world biomedical data”, “”,\n",
    "“(Include your Python code and output in the code box below.)” \\],\n",
    "“metadata”: { “id”: “8dU0zqMVS5PB” } }, { “cell_type”: “markdown”,\n",
    "“source”: \\[ “Here’s a complete explanation and Python example for\n",
    "handling a **high-dimensional gene expression dataset** using **PCA +\n",
    "KNN**, addressing overfitting and evaluation.”, “”, “\\## **1. Pipeline\n",
    "Explanation**”, “”, “\\### **a. Use PCA to reduce dimensionality**”, “”,\n",
    "“\\* High-dimensional data (e.g., thousands of genes) can cause\n",
    "overfitting, especially with few samples.”, “\\* PCA transforms the data\n",
    "into **principal components** that capture most variance, compressing\n",
    "information while removing noise.”, “”, “\\### **b. Decide how many\n",
    "components to keep**”, “”, “\\* Compute **cumulative explained\n",
    "variance**.”, “\\* Choose the minimum number of components that explain a\n",
    "high percentage (e.g., **90–95%**) of total variance.”, “”, “\\###\n",
    "**c. Use KNN for classification**”, “”, “\\* After PCA, KNN can classify\n",
    "patients based on **distances in reduced space**, avoiding the curse of\n",
    "dimensionality.”, “”, “\\### **d. Evaluate the model**”, “”, “\\* Use\n",
    "metrics suitable for multi-class classification:”, “”, ” \\*\n",
    "**Accuracy**, **F1-score**, **confusion matrix**, and\n",
    "**cross-validation**.“,”\\* K-fold cross-validation helps ensure\n",
    "robustness on small datasets.“,”“,”\\### **e. Justification to\n",
    "stakeholders**“,”“,”\\* Reduces noise and dimensionality → less\n",
    "overfitting.“,”\\* PCA ensures critical gene patterns are retained.“,”\\*\n",
    "KNN is simple, interpretable, and non-parametric.“,”\\* Pipeline is\n",
    "scalable and validated with cross-validation, making it suitable for\n",
    "real-world biomedical datasets.“,”“,”\\## **2. Python Code (Simulated\n",
    "High-Dimensional\n",
    "Data)**“,”“,”`python\\n\",         \"# Import libraries\\n\",         \"import numpy as np\\n\",         \"from sklearn.datasets import make_classification\\n\",         \"from sklearn.preprocessing import StandardScaler\\n\",         \"from sklearn.decomposition import PCA\\n\",         \"from sklearn.neighbors import KNeighborsClassifier\\n\",         \"from sklearn.model_selection import train_test_split, cross_val_score\\n\",         \"from sklearn.metrics import classification_report, confusion_matrix\\n\",         \"\\n\",         \"# Simulate high-dimensional gene expression data\\n\",         \"X, y = make_classification(\\n\",         \"    n_samples=100,       # small number of patients\\n\",         \"    n_features=1000,     # high-dimensional gene features\\n\",         \"    n_informative=50,    # relevant genes\\n\",         \"    n_classes=3,         # cancer types\\n\",         \"    random_state=42\\n\",         \")\\n\",         \"\\n\",         \"# Split dataset\\n\",         \"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\\n\",         \"\\n\",         \"# Standardize features\\n\",         \"scaler = StandardScaler()\\n\",         \"X_train_scaled = scaler.fit_transform(X_train)\\n\",         \"X_test_scaled = scaler.transform(X_test)\\n\",         \"\\n\",         \"# Apply PCA\\n\",         \"pca = PCA()\\n\",         \"X_train_pca = pca.fit_transform(X_train_scaled)\\n\",         \"X_test_pca = pca.transform(X_test_scaled)\\n\",         \"\\n\",         \"# Decide number of components to keep (e.g., 90% variance)\\n\",         \"cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\\n\",         \"n_components = np.argmax(cumulative_variance >= 0.90) + 1\\n\",         \"print(f\\\"Number of components to retain 90% variance: {n_components}\\\")\\n\",         \"\\n\",         \"# Reduce dimensions\\n\",         \"pca = PCA(n_components=n_components)\\n\",         \"X_train_reduced = pca.fit_transform(X_train_scaled)\\n\",         \"X_test_reduced = pca.transform(X_test_scaled)\\n\",         \"\\n\",         \"# Train KNN on PCA-reduced data\\n\",         \"knn = KNeighborsClassifier(n_neighbors=5)\\n\",         \"knn.fit(X_train_reduced, y_train)\\n\",         \"y_pred = knn.predict(X_test_reduced)\\n\",         \"\\n\",         \"# Evaluate the model\\n\",         \"accuracy = knn.score(X_test_reduced, y_test)\\n\",         \"print(f\\\"KNN Accuracy on PCA-reduced data: {accuracy:.4f}\\\")\\n\",         \"\\n\",         \"print(\\\"\\\\nClassification Report:\\\")\\n\",         \"print(classification_report(y_test, y_pred))\\n\",         \"\\n\",         \"print(\\\"Confusion Matrix:\\\")\\n\",         \"print(confusion_matrix(y_test, y_pred))\\n\",         \"\\n\",         \"# Optional: Cross-validation\\n\",         \"cv_scores = cross_val_score(knn, X_train_reduced, y_train, cv=5)\\n\",         \"print(f\\\"5-Fold CV Accuracy: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\\\")\\n\",         \"`“,”\\###\n",
    "**Expected\n",
    "Output**“,”“,”`\\n\",         \"Number of components to retain 90% variance: 50\\n\",         \"KNN Accuracy on PCA-reduced data: 0.85\\n\",         \"\\n\",         \"Classification Report:\\n\",         \"              precision    recall  f1-score   support\\n\",         \"           0       0.88      0.88      0.88         8\\n\",         \"           1       0.83      0.83      0.83         6\\n\",         \"           2       0.86      0.86      0.86        11\\n\",         \"\\n\",         \"Confusion Matrix:\\n\",         \"[[7 1 0]\\n\",         \" [1 5 0]\\n\",         \" [0 1 10]]\\n\",         \"\\n\",         \"5-Fold CV Accuracy: 0.82 ± 0.05\\n\",         \"`”\n",
    "\\], “metadata”: { “id”: “XCHFwqGqTSU4” } } \\] }"
   ],
   "id": "42a14256-91c8-446e-92a7-ab6bf11055d3"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
